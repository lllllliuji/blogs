(window.webpackJsonp=window.webpackJsonp||[]).push([[37],{447:function(e,t,n){e.exports=n.p+"assets/img/figure1.9b078e07.jpg"},448:function(e,t,n){e.exports=n.p+"assets/img/figure2.3ce74b76.jpg"},476:function(e,t,n){"use strict";n.r(t);var a=n(2),i=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("performance -> sharding\nfaults -> tolerance\ntolerance -> replication\nreplication -> inconsistency\nconsistency -> low performance")]),e._v(" "),t("h3",{attrs:{id:""}},[t("a",{staticClass:"header-anchor",attrs:{href:"#"}},[e._v("#")])]),e._v(" "),t("p",[e._v("the paper proposed a fairly heretical view that it was okay for the storage system to have pretty weak consistency.\ndoesn't guarantee return correct data. they take advantage of that and get better performance.\na single master.\ncynically who's going to notice the web that some vote count or something is wrong. or if you do a search on a search engine now you're gonna know like one of 20000 items is missing from the search result or they are in the wrong order.")]),e._v(" "),t("p",[e._v("what does master store ?\nfilename -> array of chunk handle (non-volatile)\nchunk handle -> list of chunk servers (volatile)\nchunk version number (non-volatile)\nprimary chunk server (volatile)\nlease time or primary (volatile)\nLog, checkpoint (non-volatile)")]),e._v(" "),t("p",[e._v("Read")]),e._v(" "),t("ol",[t("li",[e._v("filename, offset -> master")]),e._v(" "),t("li",[e._v("master -> chunk handle, list of chunk server, client may guess which replication is close to itself, choose a replication to read.\ncache this information for not disturb master again for another read.")]),e._v(" "),t("li",[e._v("client send chunk handle and offset -> chunk server")]),e._v(" "),t("li",[e._v("chunk server send data back.\nclient may divide a single read that cross two of more chunk into independent request.")])]),e._v(" "),t("p",[e._v("Write(append)\nno primary exist ?\nfind a up-to-date replicas which chunk version equals master knowed latest version, otherwise reply wrong, try later\npick the primary\nmaster increment version number, write to disk, send to primary and secondary")]),e._v(" "),t("h3",{attrs:{id:"assumption"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#assumption"}},[e._v("#")]),e._v(" Assumption")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("component failures are the norm rather than the exception.\nTherefore, constant monitoring, error detection, fault\ntolerance, and automatic recovery must be integral to the\nsystem.")])]),e._v(" "),t("li",[t("p",[e._v("files are huge by traditional standards.\nAs a result, design assumptions and parameters\nsuch as I/O operation and block sizes have to be revisited.")])]),e._v(" "),t("li",[t("p",[e._v("most files are mutated by appending new data rather than overwriting existing data.\nSome may constitute large\nrepositories that data analysis programs scan through. Some\nmay be data streams continuously generated by running applications. Some may be archival data. Some may be intermediate results produced on one machine and processed\non another, whether simultaneously or later in time. Given\nthis access pattern on huge files, appending becomes the focus of performance optimization and atomicity guarantees,\nwhile caching data blocks in the client loses its appeal.")])]),e._v(" "),t("li",[t("p",[e._v("co-designing the applications and the file system API benefits the overall system by increasing our flexibility.")])])]),e._v(" "),t("h1",{attrs:{id:"design-overview"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#design-overview"}},[e._v("#")]),e._v(" Design Overview")]),e._v(" "),t("h2",{attrs:{id:"_1-assumption"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-assumption"}},[e._v("#")]),e._v(" 1. Assumption")]),e._v(" "),t("ol",[t("li",[e._v("• The system is built from many inexpensive commodity\ncomponents that often fail. It must constantly monitor\nitself and detect, tolerate, and recover promptly from\ncomponent failures on a routine basis.")]),e._v(" "),t("li",[e._v("The system stores a modest number of large files.")]),e._v(" "),t("li",[e._v("The workloads primarily consist of two kinds of reads:\nlarge streaming reads and small random reads.Performance-conscious applications often batch\nand sort their small reads to advance steadily through\nthe file rather than go back and forth.")]),e._v(" "),t("li",[e._v("The workloads also have many large, sequential writes\nthat append data to files. Once written, files are seldom modified again. Small writes at arbitrary positions in a file are supported but do not have to be\nefficient.")]),e._v(" "),t("li",[e._v("The system must efficiently implement well-defined semantics for multiple clients that concurrently append\nto the same file.")]),e._v(" "),t("li",[e._v("High sustained bandwidth is more important than low\nlatency.")])]),e._v(" "),t("h2",{attrs:{id:"_2-architecture"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-architecture"}},[e._v("#")]),e._v(" 2. Architecture")]),e._v(" "),t("p",[e._v("A GFS cluster consists of a single master and multiple\nchunkservers and is accessed by multiple clients, as shown\nin Figure 1.\n"),t("img",{attrs:{src:n(447)+"#pic_center",alt:"figure 1"}})]),e._v(" "),t("ol",[t("li",[e._v("Files are divided into fixed-size chunks. Each chunk is\nidentified by an immutable and globally unique 64 bit chunk\nhandle assigned by the master at the time of chunk creation.")]),e._v(" "),t("li",[e._v("Chunkservers store chunks on local disks as Linux files and\nread or write chunk data specified by a chunk handle and\nbyte range.")]),e._v(" "),t("li",[e._v("For reliability, each chunk is replicated on multiple chunkservers. By default, we store three replicas, though\nusers can designate different replication levels for different\nregions of the file namespace.")])]),e._v(" "),t("p",[t("strong",[e._v("Master")])]),e._v(" "),t("ol",[t("li",[e._v("The master maintains all file system metadata. This includes the namespace, access control information, the mapping from files to chunks, and the current locations of chunks.")]),e._v(" "),t("li",[e._v("It also controls system-wide activities such as chunk lease\nmanagement, garbage collection of orphaned chunks, and\nchunk migration between chunkservers.")]),e._v(" "),t("li",[e._v("The master periodically communicates with each chunkserver in HeartBeat\nmessages to give it instructions and collect its state.")])]),e._v(" "),t("p",[t("strong",[e._v("Client")])]),e._v(" "),t("ol",[t("li",[t("p",[e._v("GFS client code linked into each application implements\nthe file system API and communicates with the master and\nchunkservers to read or write data on behalf of the application.")])]),e._v(" "),t("li",[t("p",[e._v("Clients interact with the master for metadata operations, but all data-bearing communication goes directly to\nthe chunkservers. Clients never read\nand write file data through the master.")])]),e._v(" "),t("li",[t("p",[e._v("Instead, a client asks\nthe master which chunkservers it should contact. It caches\nthis information for a limited time and interacts with the\nchunkservers directly for many subsequent operations.")])]),e._v(" "),t("li",[t("p",[e._v("Further reads\nof the same chunk require no more client-master interaction\nuntil the cached information expires or the file is reopened. In fact, the client typically asks for multiple chunks in the\nsame request and the master can also include the information for chunks immediately following those requested.")])]),e._v(" "),t("li",[t("p",[e._v("Neither the client nor the chunkserver caches file data.\nClient caches offer little benefit because most applications\nstream through huge files or have working sets too large\nto be cached.")])]),e._v(" "),t("li",[t("p",[e._v("Not having them simplifies the client and\nthe overall system by eliminating cache coherence issues.\n(***Clients do cache metadata, however.***)")])]),e._v(" "),t("li",[t("p",[e._v("Chunkservers need\nnot cache file data because chunks are stored as local files\nand so Linux’s buffer cache already keeps frequently accessed\ndata in memory.")])])]),e._v(" "),t("p",[t("em",[e._v("Chunk size 64MB")])]),e._v(" "),t("ol",[t("li",[e._v("Each chunk replica is stored as a plain\nLinux file on a chunkserver and is extended only as needed.")]),e._v(" "),t("li",[e._v("Lazy space allocation avoids wasting space due to internal\nfragmentation, perhaps the greatest objection against such a large chunk size.")])]),e._v(" "),t("p",[t("em",[e._v("advantage of large chunk size")])]),e._v(" "),t("ol",[t("li",[e._v("it reduces clients’ need to interact with the master\nbecause reads and writes on the same chunk require only\none initial request to the master for chunk location information.")]),e._v(" "),t("li",[e._v("since on a large chunk, a\nclient is more likely to perform many operations on a given\nchunk, it can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended\nperiod of time.")]),e._v(" "),t("li",[e._v("it reduces the size of the metadata\nstored on the master.")])]),e._v(" "),t("p",[t("em",[e._v("disadvantage of large chunk size")])]),e._v(" "),t("ol",[t("li",[e._v("A small file consists of a\nsmall number of chunks, perhaps just one. The chunkservers\nstoring those chunks may become hot spots if many clients\nare accessing the same file.")]),e._v(" "),t("li",[e._v("fixed this problem by storing such executables\nwith a higher replication factor and by making the batchqueue system stagger application start times.")]),e._v(" "),t("li",[e._v("A potential\nlong-term solution is to allow clients to read data from other\nclients in such situations.")])]),e._v(" "),t("p",[t("strong",[e._v("Metadata")])]),e._v(" "),t("p",[e._v("The master stores three major types of metadata:")]),e._v(" "),t("ol",[t("li",[e._v("the file and chunk namespaces. (non-volatile)")]),e._v(" "),t("li",[e._v("the mapping from files to chunks. (non-volatile)")]),e._v(" "),t("li",[e._v("the locations of each chunk’s replicas. (volatile)")])]),e._v(" "),t("p",[e._v("The first two types (namespaces and file-to-chunk mapping) are also kept persistent by\nlogging mutations to an operation log stored on the master’s local disk and replicated on remote machines.")]),e._v(" "),t("p",[e._v("The master does not store chunk location information persistently. Instead, it asks each chunkserver about its\nchunks at master startup and whenever a chunkserver joins\nthe cluster.")]),e._v(" "),t("p",[t("em",[e._v("In-Memory Data Structures")])]),e._v(" "),t("ol",[t("li",[e._v("metadata is stored in memory, master operations are\nfast.")]),e._v(" "),t("li",[e._v("it is easy and efficient for the master to\nperiodically scan through its entire state in the background.\nThis periodic scanning is used to implement chunk garbage\ncollection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space\nusage across chunkservers.")])]),e._v(" "),t("p",[e._v("The master maintains less than 64 bytes of metadata for each 64 MB\nchunk. Most chunks are full because most files contain many\nchunks, only the last of which may be partially filled. Similarly, the file namespace data typically requires less then\n64 bytes per file because it stores file names compactly using prefix compression.")]),e._v(" "),t("p",[e._v("If necessary to support even larger file systems, the cost\nof adding extra memory to the master is a small price to pay\nfor the simplicity, reliability, performance, and flexibility we gain by storing the metadata in memory.")]),e._v(" "),t("p",[t("em",[e._v("Chunk Locations")])]),e._v(" "),t("ol",[t("li",[e._v("The master does not keep a persistent record of which\nchunkservers have a replica of a given chunk.")]),e._v(" "),t("li",[e._v("It simply polls\nchunkservers for that information at startup. The master\ncan keep itself up-to-date thereafter because it controls all\nchunk placement and monitors chunkserver status with regular HeartBeat messages.")]),e._v(" "),t("li",[e._v("a chunkserver has the final word over what chunks\nit does or does not have on its own disks.")])]),e._v(" "),t("p",[t("strong",[e._v("Operation Log")])]),e._v(" "),t("ol",[t("li",[e._v("The operation log contains a historical record of critical\nmetadata changes.")]),e._v(" "),t("li",[e._v("Not only is it the\nonly persistent record of metadata, but it also serves as a\nlogical time line that defines the order of concurrent operations.")]),e._v(" "),t("li",[e._v("Files and chunks, as well as their versions, are all uniquely and eternally identified by the\nlogical times at which they were created.")])]),e._v(" "),t("p",[t("em",[e._v("Storage")])]),e._v(" "),t("ol",[t("li",[e._v("Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata\nchanges are made persistent.")]),e._v(" "),t("li",[e._v("we replicate it on\nmultiple remote machines and respond to a client operation only after flushing the corresponding log record to disk\nboth locally and remotely")]),e._v(" "),t("li",[e._v("The master batches several log\nrecords together before flushing thereby reducing the impact\nof flushing and replication on overall system throughput.")])]),e._v(" "),t("p",[t("em",[e._v("Recovery")])]),e._v(" "),t("ol",[t("li",[e._v("The master recovers its file system state by replaying the\noperation log.")]),e._v(" "),t("li",[e._v("The master checkpoints its state whenever the log\ngrows beyond a certain size so that it can recover by loading\nthe latest checkpoint from local disk and replaying only the limited number of log records after that.")]),e._v(" "),t("li",[e._v("Recovery needs only the latest complete checkpoint and\nsubsequent log files. Older checkpoints and log files can\nbe freely deleted, though we keep a few around to guard\nagainst catastrophes.")]),e._v(" "),t("li",[e._v("A failure during checkpointing does\nnot affect correctness because the recovery code detects and\nskips incomplete checkpoints.")])]),e._v(" "),t("p",[t("strong",[e._v("Consistency Model")]),t("br"),e._v("\nGFS has a relaxed consistency model.")]),e._v(" "),t("p",[t("em",[e._v("Guarantees By GFS")])]),e._v(" "),t("ol",[t("li",[e._v("File namespace mutations are atomic.They are handled exclusively by the master: namespace\nlocking guarantees atomicity and correctness;\nthe master’s operation log defines a global total order of\nthese operations.")]),e._v(" "),t("li",[e._v("The state of a file region after a data mutation depends\non the type of mutation, whether it succeeds or fails, and\nwhether there are concurrent mutations."),t("br"),e._v(" "),t("img",{attrs:{src:n(448)+"#pic_center",alt:""}}),e._v(" "),t("ol",[t("li",[e._v("consistant"),t("br"),e._v("\nA file region is consistent if all clients will\nalways see the same data, regardless of which replicas they\nread from.")]),e._v(" "),t("li",[e._v("defined"),t("br"),e._v("\nA region is defined after a file data mutation if it\nis consistent and clients will see what the mutation writes in its entirety."),t("br"),e._v("\nsequence successful mutations: defined"),t("br"),e._v("\nconcurrent successful mutations: consistant"),t("br"),e._v("\nfailed mutation: unconsistant"),t("br"),e._v("\nThe applications do not need to further distinguish\nbetween different kinds of undefined regions.")])])])]),e._v(" "),t("p",[e._v("Data mutations may be writes or record appends.")]),e._v(" "),t("ol",[t("li",[e._v("A write\ncauses data to be written at an application-specified file\noffset.")]),e._v(" "),t("li",[e._v("A record append causes data (the “record”) to be\nappended atomically at least once even in the presence of\nconcurrent mutations, but at an offset of GFS’s choosing. (In contrast, a “regular” append is merely a\nwrite at an offset that the client believes to be the current\nend of file.) The offset is returned to the client and marks\nthe beginning of a defined region that contains the record.In addition, GFS may insert padding or record duplicates in\nbetween. They occupy regions considered to be inconsistent\nand are typically dwarfed by the amount of user data.")])]),e._v(" "),t("p",[e._v("After a sequence of successful mutations, the mutated file\nregion is guaranteed to be defined and contain the data written by the last mutation."),t("br"),e._v("\nGFS achieves this by"),t("br"),e._v("\n(a) applying\nmutations to a chunk in the same order on all its replicas.")]),e._v(" "),t("p",[e._v("(b) using chunk version numbers to detect\nany replica that has become stale because it has missed mutations while its chunkserver was down.")]),e._v(" "),t("p",[e._v("Since clients cache chunk locations, they may read from a\nstale replica before that information is refreshed. This window is limited by the cache entry’s timeout and the next\nopen of the file, which purges from the cache all chunk information for that file. Moreover, as most of our files are append-only, a stale replica usually returns a premature\nend of chunk rather than outdated data. When a reader\nretries and contacts the master, it will immediately get current chunk locations.")]),e._v(" "),t("p",[e._v("GFS identifies failed\nchunkservers by regular handshakes between master and all\nchunkservers and detects data corruption by checksumming.\nOnce a problem surfaces, the data is restored\nfrom valid replicas as soon as possible. A chunk\nis lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it becomes unavailable, not corrupted: applications receive clear errors rather than corrupt data.")]),e._v(" "),t("p",[t("em",[e._v("Implications for Applications")]),t("br"),e._v("\nGFS applications can accommodate the relaxed consistency model with a few simple techniques already needed for\nother purposes:"),t("br"),e._v("\nrelying on appends rather than overwrites,\ncheckpointing, and writing self-validating, self-identifying\nrecords.")]),e._v(" "),t("p",[e._v("Readers deal with the occasional padding and duplicates as follows. Each record prepared by the writer contains extra information like checksums so that its validity can be verified. A reader can\nidentify and discard extra padding and record fragments\nusing the checksums. If it cannot tolerate the occasional\nduplicates, it can filter them out using unique identifiers in\nthe records, which are often needed anyway to name corresponding application entities such as web documents. These\nfunctionalities for record I/O (except duplicate removal) are\nin library code shared by our applications and applicable to\nother file interface implementations at Google. With that,\nthe same sequence of records, plus rare duplicates, is always\ndelivered to the record reader.")]),e._v(" "),t("h2",{attrs:{id:"system-interactions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#system-interactions"}},[e._v("#")]),e._v(" System Interactions")]),e._v(" "),t("p",[e._v("We designed the system to minimize the master’s involvement in all operations. With that background, we now describe how the client, master, and chunkservers interact to\nimplement data mutations, atomic record append, and snapshot.")]),e._v(" "),t("p",[t("strong",[e._v("Leases and Mutation Order")])])])}),[],!1,null,null,null);t.default=i.exports}}]);