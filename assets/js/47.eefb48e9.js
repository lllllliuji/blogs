(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{470:function(t,s,a){"use strict";a.r(s);var e=a(2),n=Object(e.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("作为机器学习科学家，我们的目标是发现某些模式，这些模式成功地捕捉到了我们训练集潜在总体的规律。如果成功做到了这点，即使是对以前从未遇到过的个体，\n模型也可以成功地评估风险。\n如何发现可以泛化的模式是机器学习的根本问题。")]),t._v(" "),s("p",[t._v("将模型在训练数据上拟合的比在潜在分布中更接近的现象称为"),s("em",[t._v("过拟合")]),t._v("（overfitting），\n用于对抗过拟合的技术称为"),s("em",[t._v("正则化")]),t._v("（regularization）。")]),t._v(" "),s("ul",[s("li",[t._v("训练误差（train error）是指模型在训练数据集中计算得到的误差。")]),t._v(" "),s("li",[t._v("泛化误差（generalization error）是指模型应用在同样从原始样本的分布中抽取无限多数据样本时，模型误差的期望。")])]),t._v(" "),s("p",[t._v("几个倾向于影响模型泛化的因素。")]),t._v(" "),s("ul",[s("li",[t._v("可调整参数的数量。当可调整参数的数量（有时称为"),s("em",[t._v("自由度")]),t._v("）很大时，模型往往更容易过拟合。")]),t._v(" "),s("li",[t._v("参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。")]),t._v(" "),s("li",[t._v("训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。")])]),t._v(" "),s("p",[t._v("dropout的作用大概是往神经网络中注入均值为0的噪音。"),s("br"),t._v("\n每个batch里的各个神经元的dropout的状态是相同的？")]),t._v(" "),s("p",[t._v("前几层dropout概率大一点，后几层概率小一点。dropout放激活函数之后。这是因为在深层网络中，靠近输出的层通常包含更抽象的特征，丢弃过多可能会导致网络无法有效捕捉这些高层次的模式。")]),t._v(" "),s("p",[t._v("打破对称性。"),s("br"),t._v("\n神经网络设计中的另一个问题是其参数化所固有的对称性。\n假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。\n在这种情况下，我们可以对第一层的权重"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("msup",[s("mrow",[s("mi",{attrs:{mathvariant:"bold"}},[t._v("W")])],1),s("mrow",[s("mo",[t._v("(")]),s("mn",[t._v("1")]),s("mo",[t._v(")")])],1)],1)],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\mathbf{W}^{(1)}")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.8879999999999999em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.8879999999999999em","vertical-align":"0em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{},[s("span",{staticClass:"mord textstyle uncramped"},[s("span",{staticClass:"mord mathbf",staticStyle:{"margin-right":"0.01597em"}},[t._v("W")])]),s("span",{staticClass:"vlist"},[s("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),s("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[s("span",{staticClass:"mord scriptstyle uncramped"},[s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathrm"},[t._v("1")]),s("span",{staticClass:"mclose"},[t._v(")")])])])]),s("span",{staticClass:"baseline-fix"},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("进行重排列，\n并且同样对输出层的权重进行重排列，可以获得相同的函数。\n第一个隐藏单元与第二个隐藏单元没有什么特别的区别。\n换句话说，我们在每一层的隐藏单元之间具有排列对称性。")]),t._v(" "),s("p",[t._v("请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。")])])}),[],!1,null,null,null);s.default=n.exports}}]);