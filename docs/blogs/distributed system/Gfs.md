---
title: Google File System
date: 2023-10-21
categories:
  - distributed system
tags:
  - file system
  - 6.824
---
performance -> sharding
faults -> tolerance
tolerance -> replication
replication -> inconsistency
consistency -> low performance


### 
the paper proposed a fairly heretical view that it was okay for the storage system to have pretty weak consistency.
doesn't guarantee return correct data. they take advantage of that and get better performance.
a single master.
cynically who's going to notice the web that some vote count or something is wrong. or if you do a search on a search engine now you're gonna know like one of 20000 items is missing from the search result or they are in the wrong order.


what does master store ?
filename -> array of chunk handle (non-volatile)
chunk handle -> list of chunk servers (volatile)
chunk version number (non-volatile)
primary chunk server (volatile)
lease time or primary (volatile)
Log, checkpoint (non-volatile)

Read  
1. filename, offset -> master
2. master -> chunk handle, list of chunk server, client may guess which replication is close to itself, choose a replication to read. 
   cache this information for not disturb master again for another read.
3. client send chunk handle and offset -> chunk server
4. chunk server send data back.
client may divide a single read that cross two of more chunk into independent request.


Write(append)
no primary exist ? 
find up-to-date replicas






### Assumption
1. component failures are the norm rather than the exception.
   Therefore, constant monitoring, error detection, fault
tolerance, and automatic recovery must be integral to the
system.
2. files are huge by traditional standards.
  As a result, design assumptions and parameters
such as I/O operation and block sizes have to be revisited.

3. most files are mutated by appending new data rather than overwriting existing data.
   Some may constitute large
repositories that data analysis programs scan through. Some
may be data streams continuously generated by running applications. Some may be archival data. Some may be intermediate results produced on one machine and processed
on another, whether simultaneously or later in time. Given
this access pattern on huge files, appending becomes the focus of performance optimization and atomicity guarantees,
while caching data blocks in the client loses its appeal.
4. co-designing the applications and the file system API benefits the overall system by increasing our flexibility.

