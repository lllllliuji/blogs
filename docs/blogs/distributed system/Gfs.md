---
title: Google File System
date: 2023-10-21
categories:
  - distributed system
tags:
  - file system
  - 6.824
---
performance -> sharding
faults -> tolerance
tolerance -> replication
replication -> inconsistency
consistency -> low performance


### 
the paper proposed a fairly heretical view that it was okay for the storage system to have pretty weak consistency.
doesn't guarantee return correct data. they take advantage of that and get better performance.
a single master.
cynically who's going to notice the web that some vote count or something is wrong. or if you do a search on a search engine now you're gonna know like one of 20000 items is missing from the search result or they are in the wrong order.


what does master store ?
filename -> array of chunk handle (non-volatile)
chunk handle -> list of chunk servers (volatile)
chunk version number (non-volatile)
primary chunk server (volatile)
lease time or primary (volatile)
Log, checkpoint (non-volatile)

Read  
1. filename, offset -> master
2. master -> chunk handle, list of chunk server, client may guess which replication is close to itself, choose a replication to read. 
   cache this information for not disturb master again for another read.
3. client send chunk handle and offset -> chunk server
4. chunk server send data back.
client may divide a single read that cross two of more chunk into independent request.


Write(append)
no primary exist ? 
find a up-to-date replicas which chunk version equals master knowed latest version, otherwise reply wrong, try later
pick the primary
master increment version number, write to disk, send to primary and secondary







### Assumption
1. component failures are the norm rather than the exception.
   Therefore, constant monitoring, error detection, fault
tolerance, and automatic recovery must be integral to the
system.
2. files are huge by traditional standards.
  As a result, design assumptions and parameters
such as I/O operation and block sizes have to be revisited.

3. most files are mutated by appending new data rather than overwriting existing data.
   Some may constitute large
repositories that data analysis programs scan through. Some
may be data streams continuously generated by running applications. Some may be archival data. Some may be intermediate results produced on one machine and processed
on another, whether simultaneously or later in time. Given
this access pattern on huge files, appending becomes the focus of performance optimization and atomicity guarantees,
while caching data blocks in the client loses its appeal.
4. co-designing the applications and the file system API benefits the overall system by increasing our flexibility.

# Design Overview
## 1. Assumption
1. • The system is built from many inexpensive commodity
components that often fail. It must constantly monitor
itself and detect, tolerate, and recover promptly from
component failures on a routine basis.
2. The system stores a modest number of large files.
3. The workloads primarily consist of two kinds of reads:
large streaming reads and small random reads.Performance-conscious applications often batch
and sort their small reads to advance steadily through
the file rather than go back and forth.
4. The workloads also have many large, sequential writes
that append data to files. Once written, files are seldom modified again. Small writes at arbitrary positions in a file are supported but do not have to be
efficient.
5. The system must efficiently implement well-defined semantics for multiple clients that concurrently append
to the same file.
6. High sustained bandwidth is more important than low
latency.

## 2. Architecture
A GFS cluster consists of a single master and multiple
chunkservers and is accessed by multiple clients, as shown
in Figure 1.
![figure 1](./../../.vuepress/public/gfs/figure1.jpg#pic_center)
1. Files are divided into fixed-size chunks. Each chunk is
identified by an immutable and globally unique 64 bit chunk
handle assigned by the master at the time of chunk creation.
2. Chunkservers store chunks on local disks as Linux files and
read or write chunk data specified by a chunk handle and
byte range. 
3. For reliability, each chunk is replicated on multiple chunkservers. By default, we store three replicas, though
users can designate different replication levels for different
regions of the file namespace.

**Master**

1. The master maintains all file system metadata. This includes the namespace, access control information, the mapping from files to chunks, and the current locations of chunks.
2. It also controls system-wide activities such as chunk lease
management, garbage collection of orphaned chunks, and
chunk migration between chunkservers. 
3. The master periodically communicates with each chunkserver in HeartBeat
messages to give it instructions and collect its state.

**Client**
1. GFS client code linked into each application implements
the file system API and communicates with the master and
chunkservers to read or write data on behalf of the application.
2. Clients interact with the master for metadata operations, but all data-bearing communication goes directly to
the chunkservers. Clients never read
and write file data through the master. 

3. Instead, a client asks
the master which chunkservers it should contact. It caches
this information for a limited time and interacts with the
chunkservers directly for many subsequent operations.

4. Further reads
of the same chunk require no more client-master interaction
until the cached information expires or the file is reopened. In fact, the client typically asks for multiple chunks in the
same request and the master can also include the information for chunks immediately following those requested.
5. Neither the client nor the chunkserver caches file data.
Client caches offer little benefit because most applications
stream through huge files or have working sets too large
to be cached. 
6. Not having them simplifies the client and
the overall system by eliminating cache coherence issues.
(***Clients do cache metadata, however.***)
7. Chunkservers need
not cache file data because chunks are stored as local files
and so Linux’s buffer cache already keeps frequently accessed
data in memory.

*Chunk size 64MB*
1. Each chunk replica is stored as a plain
Linux file on a chunkserver and is extended only as needed.
2. Lazy space allocation avoids wasting space due to internal
fragmentation, perhaps the greatest objection against such a large chunk size.

*advantage of large chunk size*
 1. it reduces clients’ need to interact with the master
because reads and writes on the same chunk require only
one initial request to the master for chunk location information.
2. since on a large chunk, a
client is more likely to perform many operations on a given
chunk, it can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended
period of time. 
3.  it reduces the size of the metadata
stored on the master.

*disadvantage of large chunk size*
1. A small file consists of a
small number of chunks, perhaps just one. The chunkservers
storing those chunks may become hot spots if many clients
are accessing the same file. 
2. fixed this problem by storing such executables
with a higher replication factor and by making the batchqueue system stagger application start times.
3. A potential
long-term solution is to allow clients to read data from other
clients in such situations.

**Metadata**

The master stores three major types of metadata: 
1. the file and chunk namespaces. (non-volatile)
2. the mapping from files to chunks. (non-volatile)
3. the locations of each chunk’s replicas. (volatile)

The first two types (namespaces and file-to-chunk mapping) are also kept persistent by
logging mutations to an operation log stored on the master’s local disk and replicated on remote machines.

The master does not store chunk location information persistently. Instead, it asks each chunkserver about its
chunks at master startup and whenever a chunkserver joins
the cluster.

*In-Memory Data Structures*  
1. metadata is stored in memory, master operations are
fast. 
2. it is easy and efficient for the master to
periodically scan through its entire state in the background.
This periodic scanning is used to implement chunk garbage
collection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space
usage across chunkservers.

The master maintains less than 64 bytes of metadata for each 64 MB
chunk. Most chunks are full because most files contain many
chunks, only the last of which may be partially filled. Similarly, the file namespace data typically requires less then
64 bytes per file because it stores file names compactly using prefix compression.

If necessary to support even larger file systems, the cost
of adding extra memory to the master is a small price to pay
for the simplicity, reliability, performance, and flexibility we gain by storing the metadata in memory.

*Chunk Locations*  
1. The master does not keep a persistent record of which
chunkservers have a replica of a given chunk. 
2. It simply polls
chunkservers for that information at startup. The master
can keep itself up-to-date thereafter because it controls all
chunk placement and monitors chunkserver status with regular HeartBeat messages. 
3. a chunkserver has the final word over what chunks
it does or does not have on its own disks.


**Operation Log**  
1. The operation log contains a historical record of critical
metadata changes. 
2. Not only is it the
only persistent record of metadata, but it also serves as a
logical time line that defines the order of concurrent operations.
3. Files and chunks, as well as their versions, are all uniquely and eternally identified by the
logical times at which they were created.

*Storage*
1. Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata
changes are made persistent.
2. we replicate it on
multiple remote machines and respond to a client operation only after flushing the corresponding log record to disk
both locally and remotely
3. The master batches several log
records together before flushing thereby reducing the impact
of flushing and replication on overall system throughput.

*Recovery*
1. The master recovers its file system state by replaying the
operation log. 
2. The master checkpoints its state whenever the log
grows beyond a certain size so that it can recover by loading
the latest checkpoint from local disk and replaying only the limited number of log records after that.
3. Recovery needs only the latest complete checkpoint and
subsequent log files. Older checkpoints and log files can
be freely deleted, though we keep a few around to guard
against catastrophes.
4. A failure during checkpointing does
not affect correctness because the recovery code detects and
skips incomplete checkpoints.


**Consistency Model**  
GFS has a relaxed consistency model.

*Guarantees By GFS*
1. File namespace mutations are atomic.They are handled exclusively by the master: namespace
locking guarantees atomicity and correctness;
the master’s operation log defines a global total order of
these operations.
2. The state of a file region after a data mutation depends
on the type of mutation, whether it succeeds or fails, and
whether there are concurrent mutations.  
![](../../.vuepress/public/gfs/figure2.jpg#pic_center)
   1. consistant  
    A file region is consistent if all clients will
always see the same data, regardless of which replicas they
read from. 
   2.  defined  
    A region is defined after a file data mutation if it
is consistent and clients will see what the mutation writes in its entirety.  
sequence successful mutations: defined  
concurrent successful mutations: consistant  
failed mutation: unconsistant  
The applications do not need to further distinguish
between different kinds of undefined regions.

Data mutations may be writes or record appends.  
1. A write
causes data to be written at an application-specified file
offset.
2. A record append causes data (the “record”) to be
appended atomically at least once even in the presence of
concurrent mutations, but at an offset of GFS’s choosing. (In contrast, a “regular” append is merely a
write at an offset that the client believes to be the current
end of file.) The offset is returned to the client and marks
the beginning of a defined region that contains the record.In addition, GFS may insert padding or record duplicates in
between. They occupy regions considered to be inconsistent
and are typically dwarfed by the amount of user data.

After a sequence of successful mutations, the mutated file
region is guaranteed to be defined and contain the data written by the last mutation.  
GFS achieves this by   
(a) applying
mutations to a chunk in the same order on all its replicas.  

(b) using chunk version numbers to detect
any replica that has become stale because it has missed mutations while its chunkserver was down.


Since clients cache chunk locations, they may read from a
stale replica before that information is refreshed. This window is limited by the cache entry’s timeout and the next
open of the file, which purges from the cache all chunk information for that file. Moreover, as most of our files are append-only, a stale replica usually returns a premature
end of chunk rather than outdated data. When a reader
retries and contacts the master, it will immediately get current chunk locations.  


 GFS identifies failed
chunkservers by regular handshakes between master and all
chunkservers and detects data corruption by checksumming.
 Once a problem surfaces, the data is restored
from valid replicas as soon as possible. A chunk
is lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it becomes unavailable, not corrupted: applications receive clear errors rather than corrupt data.

*Implications for Applications*  
GFS applications can accommodate the relaxed consistency model with a few simple techniques already needed for
other purposes:  
relying on appends rather than overwrites,
checkpointing, and writing self-validating, self-identifying
records.

Readers deal with the occasional padding and duplicates as follows. Each record prepared by the writer contains extra information like checksums so that its validity can be verified. A reader can
identify and discard extra padding and record fragments
using the checksums. If it cannot tolerate the occasional
duplicates, it can filter them out using unique identifiers in
the records, which are often needed anyway to name corresponding application entities such as web documents. These
functionalities for record I/O (except duplicate removal) are
in library code shared by our applications and applicable to
other file interface implementations at Google. With that,
the same sequence of records, plus rare duplicates, is always
delivered to the record reader.

## System Interactions
We designed the system to minimize the master’s involvement in all operations. With that background, we now describe how the client, master, and chunkservers interact to
implement data mutations, atomic record append, and snapshot.

**Leases and Mutation Order**

