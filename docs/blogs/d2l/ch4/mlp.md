---
title: mlp
date: 2024-11-08
categories:
  - deep learning
tags:
  - python
---

作为机器学习科学家，我们的目标是发现某些模式，这些模式成功地捕捉到了我们训练集潜在总体的规律。如果成功做到了这点，即使是对以前从未遇到过的个体，
模型也可以成功地评估风险。
如何发现可以泛化的模式是机器学习的根本问题。

将模型在训练数据上拟合的比在潜在分布中更接近的现象称为*过拟合*（overfitting），
用于对抗过拟合的技术称为*正则化*（regularization）。

* 训练误差（train error）是指模型在训练数据集中计算得到的误差。
* 泛化误差（generalization error）是指模型应用在同样从原始样本的分布中抽取无限多数据样本时，模型误差的期望。


几个倾向于影响模型泛化的因素。

* 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
* 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
* 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

前几层dropout概率大一点，后几层概率小一点。dropout放激活函数之后。


打破对称性。  
神经网络设计中的另一个问题是其参数化所固有的对称性。
假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。
在这种情况下，我们可以对第一层的权重$\mathbf{W}^{(1)}$进行重排列，
并且同样对输出层的权重进行重排列，可以获得相同的函数。
第一个隐藏单元与第二个隐藏单元没有什么特别的区别。
换句话说，我们在每一层的隐藏单元之间具有排列对称性。

请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。

